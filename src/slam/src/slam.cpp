/*************************************************
 *  slam.cpp
 *  
 *  Uses error measurements from the pose estimate
 *  generated by pose_estimate.cpp to control the steering
 *  of the vehicle using a PID controller.
 *
 *  Subscribers: pose_estimate
 *  Publishers: N/A
 *
 *  Author: Zachary Kendrick
 ************************************************/

#include <ros/ros.h>
#include <image_transport/image_transport.h>
#include <sstream>
#include <iostream>
#include <cv_bridge/cv_bridge.h>
#include <opencv/cv.h>
#include <opencv/highgui.h>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/objdetect/objdetect.hpp>
#include <visualization_msgs/Marker.h>
#include <image_geometry/pinhole_camera_model.h>
#include <opencv2/features2d.hpp>
#include <opencv2/video/tracking.hpp>
#include <typeinfo>


using namespace cv;
using namespace std;
using namespace ros;

// ORB feature parameters
const static int N_FEATURES = 1200;
const static float SCALE_FACTOR = 1.2f;
const static int N_LEVELS = 8;
const static int EDGE_THRESHOLD = 31;
const static int FIRST_LEVEL = 0;
const static int WTA_K = 2;
const static int SCORE_TYPE = ORB::HARRIS_SCORE;
const static int PATCH_SIZE = 31;
const static int FAST_THRESHOLD = 20;
const static Scalar COLOR = Scalar(139,0,139);

// optical flow parameters																						
const static Size  WIN_SIZE = Size(21,21);
const static int MAX_LEVEL = 3;
const static TermCriteria TERM_CRIT = TermCriteria(TermCriteria::COUNT+TermCriteria::EPS, 30, 0.01);
const static int FLAGS = 0;
const static double MIN_EIG_THRESHOLD = 1e-4;

// boolean for detecting new features
static bool new_detection = true;

// feature threshold
const static int FEATURE_THRESHOLD = 100;

// ORB feature detector
static Ptr<ORB> detector;

// previous points and frame
vector<Point2f> previous_points;
Mat previous_frame;

// rotation and translation 
Mat R_star = Mat::eye(3, 3, CV_64F);
Mat t_star = Mat::zeros(3, 1, CV_64F);

// focal length and center of optical axis
// const static double FOCAL = 313.96138047825696;
// const static cv::Point2d PP(304.05329341210603, 225.79847068204063);

const static double FOCAL = 309.58086449;
const static cv::Point2d PP(332.29985864, 240.61274041);

// find new features every n frames
static int num_frames = 0;
const static int FIND_NEW_FEATURES = 5;

// optical flow mask
Mat flow_mask; 

// window displaying the path of the vehicle
Mat traj = Mat::zeros(600, 600, CV_8UC3);

// functions
void getRotationAndTranslation(vector<Point2f>& previous_points, vector<Point2f>& current_points, Mat& R_star, Mat& t_star); 
void featureDetection(Mat frame, vector<Point2f>& previous_points, vector<KeyPoint>& keypoints);
bool featureTracking(Mat previous_frame, Mat current_frame, vector<Point2f>& previous_points, vector<Point2f>& current_points, vector<uchar>& status);
void getOpticalFlowMask(Mat flow_mask, vector<Point2f>& previous_points, vector<Point2f>& current_points, vector<Point2f>& found_points, vector<uchar>& status);
float getAbsoluteScale(Mat t_star, Mat t);


/*
   Function:
      Call back function that reads an image from the
      "raw_image" topic and finds lines using hough 
      transforms that make up the inner and outer
      markings of the lanes.
   Parameters:
      const sensor_msgs::ImageConstPtr& msg
   Publishes:
      Line list with id=2 (white lanes) and id=3 (yellow lanes)
*/

void imageLanePoints(const sensor_msgs::ImageConstPtr& msg)
{
  try
  {

    // current frame and points
    Mat current_frame;
    vector<Point2f> current_points;
    vector<uchar> status;

    // convert image to grayscale
    Mat RGB_frame = cv_bridge::toCvShare(msg, "bgr8")->image;
    cvtColor(RGB_frame, current_frame, COLOR_RGB2GRAY);

    // detect new features
    if (num_frames == FIND_NEW_FEATURES || new_detection) {

    	// new key points and optical flow mask
    	vector<KeyPoint> keypoints;
		flow_mask = Mat::zeros(current_frame.rows, current_frame.cols, CV_8UC3);

    	// detect features
    	featureDetection(current_frame, current_points, keypoints);
    	new_detection = false;
    	num_frames = 0;

		// reset the points and frames
		previous_frame = current_frame;
		previous_points = current_points;

    	drawKeypoints(RGB_frame, keypoints, RGB_frame, COLOR);
    }

    // track old features
    else {

		vector<Point2f> found_points;
    	new_detection = featureTracking(previous_frame, current_frame, previous_points, current_points, status);
    	getOpticalFlowMask(flow_mask, previous_points, current_points, found_points, status);
    	num_frames++;

    	// get new rotation and translation from consecutive frames
		getRotationAndTranslation(previous_points, current_points, R_star, t_star);



		int x = int(t_star.at<double>(0)) + 300;
		int y = int(t_star.at<double>(2)) + 300;
		circle(traj, Point(x, y) ,1, CV_RGB(255,0,0), 2);

		rectangle(traj, Point(10, 30), Point(550, 50), CV_RGB(0,0,0), CV_FILLED);

		imshow("Trajectory", traj);

    	// reset the points and frames
		previous_frame = current_frame;
		previous_points = current_points;
    }

    add(RGB_frame, flow_mask, RGB_frame);

    // display frames
    imshow("ORB Features", RGB_frame);
    waitKey(100);
  }
  catch (cv_bridge::Exception& e)
  {
    ROS_ERROR("Could not convert from '%s' to 'bgr8'.", msg->encoding.c_str());
  }
}





void getRotationAndTranslation(vector<Point2f>& previous_points, vector<Point2f>& current_points, Mat& R_star, Mat& t_star) {
	Mat E, R, t, mask;
	int scale = 2;

	// find rotation and translation from essential matrix
	E = findEssentialMat(previous_points, current_points, FOCAL, PP, RANSAC, 0.999, 1.0, mask);
	recoverPose(E, previous_points, current_points, R, t, FOCAL, PP, mask);

	t_star = t_star + scale*(R_star*t);
	R_star = R*R_star;
}



void getOpticalFlowMask(Mat flow_mask, vector<Point2f>& previous_points, vector<Point2f>& current_points, vector<Point2f>& found_points, vector<uchar>& status) {

	int rows = flow_mask.cols;
	int cols = flow_mask.rows;
	for(int i=0; i < current_points.size(); ++i) {
			line(flow_mask, previous_points.at(i), current_points.at(i), Scalar::all(255), 1);
			found_points.push_back(current_points.at(i));
	}

	std::cout << "Found " << found_points.size() << " Keypoints " << endl;
	
	// set the current points to the found points
	current_points = found_points;

}




void featureDetection(Mat frame, vector<Point2f>& current_points, vector<KeyPoint>& keypoints) {

	// int fast_threshold = 16;
	// bool nonmaxSuppression = true;
	// FAST(frame, keypoints, fast_threshold, nonmaxSuppression);

	// get ORB key points
    detector->detect(frame, keypoints);
    std::cout << "Detected " << keypoints.size() << " Keypoints " << endl;
       
    // convert key points to points for optical flow
	KeyPoint::convert(keypoints, current_points, vector<int>());
}




bool featureTracking(Mat previous_frame, Mat current_frame, vector<Point2f>& previous_points, vector<Point2f>& current_points, vector<uchar>& status) {

	// error message
	vector<float> err;

	// calculate optical flow
	calcOpticalFlowPyrLK(
	previous_frame,
	current_frame,
	previous_points,
	current_points, //found_points,
	status,
	err,
	WIN_SIZE,
	MAX_LEVEL,
	TERM_CRIT,
	FLAGS,
	MIN_EIG_THRESHOLD);



	// getting rid of points for which the KLT tracking failed or those who have gone outside the frame
	int correction = 0;
	for( int i=0; i<status.size(); i++) {
		  Point2f pt = current_points.at(i-correction);
	 	if ((status.at(i) == 0)||(pt.x<0)||(pt.y<0)) {
	 		  if((pt.x<0)||(pt.y<0)) {
	 		  	status.at(i) = 0;
	 		  }
	 		  previous_points.erase(previous_points.begin() + (i - correction));
	 		  current_points.erase(current_points.begin() + (i - correction));
	 		  correction++;
	 	}
	}

	// check that enough features were found
	return (current_points.size() < FEATURE_THRESHOLD); 

}



int main(int argc, char **argv) {
	ros::init(argc, argv, "slam");
	ros::NodeHandle nh;
	cv::namedWindow("ORB Features");
	cv::startWindowThread();
	image_transport::ImageTransport it(nh);

	// ORB feature detector
	detector = ORB::create(
		N_FEATURES,
		SCALE_FACTOR,
		N_LEVELS,
		EDGE_THRESHOLD,
		FIRST_LEVEL,
		WTA_K,
		SCORE_TYPE,
		PATCH_SIZE,
		FAST_THRESHOLD);

	// subscribe to the "raw_image" topic
	image_transport::Subscriber sub = it.subscribe("raw_image", 1, imageLanePoints);

	// publish the "lane_lines"
	// lane_lines_pub = nh.advertise<visualization_msgs::Marker>("visualization_marker", 1);
	ros::Rate r(30);
	ros::spin();
	cv::destroyWindow("ORB Features");
}